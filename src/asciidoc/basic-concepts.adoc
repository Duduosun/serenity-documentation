To get the most out of Serenity BDD, it is useful to understand some of the basic Serenity BDD behind Behaviour Driven Development and Automated Acceptance Testing. Serenity BDD is commonly used for both Automated Acceptance Tests and Regression Tests, and the principles discussed here apply, with minor variations, to both.

Behaviour Driven Development or BDD, is a development approach where team members explore, and build a shared understanding of application requirements through conversations around examples. In Agile teams practicing BDD, this is often done before or early on in a sprint, in a special meeting sometimes called "the three amigos" or "the three-way handshake", where (at least) a BA (Business Analyst, Product Owner/Manager), a developer and a tester get together to work through examples from the acceptance criteria. The examples being discussed are concrete illustrations of how the system should work, or how a user might use a feature. These examples help provoke discussion, uncovering assumptions and omissions that would have otherwise lead the development team into error further down the track.

Let's look at an example. Suppose we are building the shopping cart component of an online craft sales site.
In Agile terms, the corresponding user story might look like this:

[source,gherkin]
----
In order to make the most appropriate purchase decisions
As a buyer
I want to be able to place items I want to buy in a virtual cart before placing my order
----

If we were implementing this story, we would typically define a set of acceptance criteria to flesh out and understand the requirements. For example, we might have the following criteria in our list of acceptance criteria:

  - Show total price for all items
  - Show line item prices
  - Show shipping costs
  - ...

If we were using a Behaviour-Driven-Development approach, we might express these requirements in a more formal form, like the following:

[source,gherkin]
----
Scenario: Show shipping cost for an item in the shopping cart
Given I have searched for 'docking station'
And I have selected a matching item
When I add it to the cart
Then the shipping cost should be included in the total price
----

This https://www.agilealliance.org/glossary/gwt-2/[Given When Then] format is widely used for acceptance tests in Agile projects.

[[fig-test-report]]
.A test report generated by Serenity
image::serenity-test-report.png[]

Note how this scenario is deliberately pitched at a fairly high level, in business terms, describing the business motivations behind the feature without committing to a particular implementation.

When a tester or a developer automates and executes this scenario, or a BA reviews the results, they will often want to see a bit more detail. For example, the tester will want to see how the screens played out (if it's a web test), what test data was used and so on. And the BA might want to see what the screens look like for each step.

Serenity BDD does below for you

  - Makes it easy to write, execute, and report on automated acceptance tests in terms like this, that BAs and testers as well as developers can relate to.
  - Structure your automated acceptance tests into steps and sub-steps like the ones illustrated above. This tends to make the tests clearer, more flexible and easier to maintain.
  - When the tests are executed, Serenity produces illustrated, narrative-style reports like this:

[[fig-aggregate-report]]
.An aggregate report generated by Serenity
image::serenity-aggregate-report.png[]

Serenity BDD also gives you a broader picture, helping you see where individual scenarios fit into the overall set of product requirements. It helps you see not only the current state of the tests, but also what requirements have been (and have not been) tested (see <<fig-aggregate-report>>).

Serenity BDD is also commonly used for automated http://en.wikipedia.org/wiki/Regression_testing[Regression Tests]. Whereas BDD Acceptance Tests are defined very early on in the piece, before development starts, Regression Tests involve an existing system. Other than that, the steps involved in defining and automating the tests are very similar.

When it comes to implementing the tests themselves, Serenity BDD also provides many features that make it easier, faster and cleaner to write clear, maintainable tests. This is particularly true for automated web tests using WebDriver, but Serenity BDD also caters for non-web tests as well. Serenity BDD plays well with JUnit as well as more specialized BDD frameworks such as Cucumber and JBehave.

=== Detailed description of aggregation reports

Serenity BDD aggregation report can be organised by using features, stories, steps, scenarios/tests. When you use different frameworks with Serenity BDD it is possible that the same things will have different definitions. For instance, `examples` in JBehave/Cucumber has almost same meaning as `Test Data` in Junit, or `scenario` in JBehave/Cucumber is the same as `test` in JUnit. Also *Test* is a synonym of *Acceptance Criteria*. Here is introduced some short example in order to describe Serenity BDD report. The way of creating and organising the whole test process you can find in next chapters.

Our example contains 2 features with a few stories. Each story can contain one or more scenarios, each scenario consists of one or more steps and some examples. The default amount of examples is 1.

Sample:

----
Feature:Definition
		Story: Look for definition
			Scenario (or Test, or Acceptance Criteria): Looking for definition: pass
                examples: 3
                steps: 3
			Scenario: Looking for definition with incorrect symbols: @Ignore
                examples: 4
                steps: 3
			Scenario: Looking for not existed definition: failed
                examples: 2
                steps: 3
		Story: Update a definition
			Scenario: Updating a definition: some internal error
                examples: 5
                steps: 3

Feature:Petstore
		Story: Remove a pet
			Scenario: Removing a pet: @Skip
			  	steps: 5
			Scenario: Removing multiple pets: @Pending
			  	steps: 6
		Story: Update a pet
			Scenario: Updating a pet: @Ignore
			  	steps: 4
		Story: Add a pet
			Scenario: Adding a pet: pass
			  	steps: 3
----

Scenarios Removing a pet, Removing multiple pets, Updating a pet, Adding a pet are defined without usage of any examples.
Scenarios Looking for definition, Looking for definition with incorrect symbols, Looking for not existed definition, Updating a definition are defined with examples.
Scenario Removing a pet is marked to be skipped.
Scenarios Updating a pet, Looking for definition with incorrect symbols are marked to be ignored.
Scenarios Looking for definition, Adding a pet should pass.
Scenario Looking for not existed definition should not be passed.
Scenario Updating a definition should throw unexpected exception during execution.
Scenario Removing multiple pets is marked to be pending.

After running all these tests (doesn't matter if you use JBehave, Cucumber or JUnit) you will receive aggregation report, that will look similar to the structure of tests:

[[basic-concepts-detailed-test-count]]
.Serenity BDD report for example test on tab Test Count
image::basic-concepts-detailed-test-count.png[]


Report contains test results of all executed scenarios, and consists of the next tabs:

*Overall Test Results*:: General info about provided features/components stories in this test. Also represents statistics of passed/ignored/skipped/failed tests based on their amount and examples.

*Requirements*:: Detailed info about statistics based on Features, Stories and Acceptance Criteria

*Features*:: Summary table of all Features

*Stories*:: Summary table with statistics of stories

==== Tab Overall Test Results

Here you can find almost all information about executed tests. It consists of next sub-tabs:

*Test Count*:: Summary page of all general statistics and info, based on amount of scenarios and used examples.

*Weighted Tests*:: Summary page of all general statistics and info, weighted by scenarios size in steps.

There is also general information about executed tests:

----
8 test scenarios (15 tests in all, including 10 rows of test data)
4 passes, 1 pending, 2 failed, 5 with errors, 0 compromised, 2 ignored, 1 skipped
----


*ignored* = 2 - amount of all scenarios which are marked to be ignored. To get this number Serenity counts scenarios with @Ignored mark.

*skipped* = 1 - amount of all scenarios which are marked to be skipped. To get this number Serenity counts scenarios with @Skipped mark.

*with errors* = 5 - amount of all scenarios which throw some unexpected exception during execution. To get this number Serenity counts scenarios with Error mark or examples of those scenarios if provided.

*failed* = 2 - amount of all scenarios which fail. To get this number Serenity counts failed scenarios or examples of those scenarios if provided.

*pending* = 1 - amount of all scenarios which are marked to be pending. To get this number Serenity counts scenarios with @Pending mark or examples of those scenarios if provided.

*passes* = 4 - amount of all passed scenarios. To get this number Serenity counts passed scenarios or examples of those scenarios if provided.

*rows of test data* = 10 - amount of all examples from scenarios witch are used in this report, including skipped scenarios but without ignored scenarios. To get this number Serenity counts examples of those scenarios if provided. In our case there are 3 of such scenarios: with 2, 3 and 5 examples.

*tests in all* = 15 - sum of "ignored", "skipped", "with errors", "failed", "pending", "passes" values

*test scenarios* = 8 - amount of all scenarios in this test. In our sample there are 8 scenarios.


===== Sub-Tab Test Count

As you can see on <<basic-concepts-detailed-test-count>>, it contains next elements: Pie Chart, Test Result Summary table, Related Tags table and Test table.

*Test Result Summary*.
This table contains more detailed statistics than short summary above.

Row *Automated* contains automated tests.

- *Ignored* - count of automated tests which are marked to be ignored.To get this number Serenity counts scenarios are marked as @Ignored.

- *Percent of ignored tests* - percentage of *Ignored* tests to *tests in all*. In our case there are 2 such scenario, and it is 13% of 15.

- *Pending* - amount of all scenarios are marked to be pending. To get this number Serenity counts @Pending scenarios or examples of those scenarios if provided.

- *Percent of pending tests* - percentage of *Pending* tests to *tests in all*. In our case there is 1 such scenario, and it is 7% from 15.

- *Fail* - amount of all scenarios which failed and scenarios with errors. In our case there is 1 failed scenario with 2 examples and 1 error scenario with 5 examples - 7 as a result.

- *Percent of fail tests* - percentage of *Fail* tests to *tests in all*. In our case there are 7 scenarios/examples and it is 47% from 15.

- *Pass* - amount of passed scenarios. In our case there are 2 such scenario: one without examples, and second with 3 examples - 4 as a result.

- *Percent of passed tests* - percentage of *Pass* tests to *tests in all*. In our case there are 4 scenarios/examples and it is 27% from 15.

- *Total* - equal to *tests in all*

Row *Manual* contains manual tests. In order to execute test manually you should use @Manual annotation, idea the same as for Automated row. Generally, it is possible to use it to mark scenarios (@Manual on scenario level) or all scenarios in story (@manual on Story level). The @Manual annotation is not designed to be defined for an individual step within a test, but only for the whole test.

Row *Total* should contains summary for each column.

===== Sub-Tab Weighted tests

This sub tab contains rest results weighted by test size in steps

[[basic-concepts-detailed-weighted-tests]]
.Serenity BDD report for example test on tab Weighted tests
image::basic-concepts-detailed-weighted-tests.png[]

==== Tab Requirements

On this tab all tests results are organized as requirements

[[basic-concepts-detailed-requirements]]
.Serenity BDD report for test structure on tab with Requirements
image::basic-concepts-detailed-requirements.png[]

==== Tab Features

On this tab all tests results are organized as features. In our example we have 2 features

[[basic-concepts-detailed-test-count]]
.Serenity BDD report for test structure on tab with Features
image::basic-concepts-detailed-features.png[]

==== Tab Stories

There are stories on this tab. in our example - there are 5 stories.

[[basic-concepts-detailed-test-count]]
.Serenity BDD report for test structure on tab with Stories
image::basic-concepts-detailed-stories.png[]

==== Filtering in Serenity Reports

To provide a better user experience, there is available a filtering feature in Serenity BDD aggregated reports. It makes much easier to find particular subset of tests by names, features, etc.

For example you have next tests:
[[subset-of-tests-for-filtering]]
.Serenity BDD report example for filtering
image::subset-of-tests-for-filtering.png[]


It is easy to filter some of them with starting typing its name in filter field:
[[filtered-tests-for-filtering]]
.Serenity BDD report example with applied filter
image::filtered-tests-for-filtering.png[]

Filtering feature enabled for almost all main pages of serenity report.